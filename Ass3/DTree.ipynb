{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T22:26:35.432271Z",
     "start_time": "2018-04-02T22:26:34.630490Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import statistics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T22:26:37.119640Z",
     "start_time": "2018-04-02T22:26:36.089186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sizes are  Train: 27000 , Validation: 3000 , Test: 7000\n"
     ]
    }
   ],
   "source": [
    "## The possible attributes in the data with the prediction at index 0. Smaller names for brevity.\n",
    "attributes = [\"rich\",\"age\",\"wc\",\"fnlwgt\",\"edu\",\"edun\",\"mar\",\"occ\",\"rel\",\"race\",\"sex\",\"capg\",\"canpl\",\"hpw\",\"nc\"]\n",
    "\n",
    "## Get the encoding of the csv file by replacing each categorical attribute value by its index.\n",
    "wc_l = \"Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked\".split(\", \")\n",
    "edu_l = \"Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool\".split(\", \")\n",
    "mar_l = \"Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse\".split(\", \")\n",
    "occ_l = \"Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces\".split(\", \")\n",
    "rel_l = \"Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\".split(\", \")\n",
    "race_l = \"White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\".split(\", \")\n",
    "sex_l = \"Female, Male\".split(\", \")\n",
    "nc_l = \"United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands\".split(\", \")\n",
    "encode = {\n",
    "    \"rich\"   : {\"0\":0,\"1\":1},\n",
    "    \"wc\"     : {wc_l[i]:i for i in range(len(wc_l))},\n",
    "    \"edu\"    : {edu_l[i]:i for i in range(len(edu_l))},\n",
    "    \"mar\"    : {mar_l[i]:i for i in range(len(mar_l))},\n",
    "    \"occ\"    : {occ_l[i]:i for i in range(len(occ_l))},\n",
    "    \"rel\"    : {rel_l[i]:i for i in range(len(rel_l))},\n",
    "    \"race\"   : {race_l[i]:i for i in range(len(race_l))},\n",
    "    \"sex\"    : {sex_l[i]:i for i in range(len(sex_l))},\n",
    "    \"nc\"     : {nc_l[i]:i for i in range(len(nc_l))},\n",
    "    }\n",
    "\n",
    "def medians(file):\n",
    "    \"\"\"\n",
    "    Given a csv file, find the medians of the categorical attributes for the whole data.\n",
    "    params(1): \n",
    "        file : string : the name of the file\n",
    "    outputs(6):\n",
    "        median values for the categorical columns\n",
    "    \"\"\"\n",
    "    fin = open(file,\"r\")\n",
    "    reader = csv.reader(fin)\n",
    "    age, fnlwgt, edun, capg, capl, hpw = ([] for i in range(6))\n",
    "    total = 0\n",
    "    for row in reader:\n",
    "        total+=1\n",
    "        if(total==1):\n",
    "            continue\n",
    "        l = [x.lstrip().rstrip() for x in row]\n",
    "        age.append(int(l[0]));\n",
    "        fnlwgt.append(int(l[2]));\n",
    "        edun.append(int(l[4]));\n",
    "        capg.append(int(l[10]));\n",
    "        capl.append(int(l[11]));\n",
    "        hpw.append(int(l[12]));\n",
    "    fin.close()\n",
    "    return(statistics.median(age),statistics.median(fnlwgt),statistics.median(edun),statistics.median(capg),statistics.median(capl),statistics.median(hpw))\n",
    "\n",
    "def preprocess(file):\n",
    "    \"\"\"\n",
    "    Given a file, read its data by encoding categorical attributes and binarising continuos attributes based on median.\n",
    "    params(1): \n",
    "        file : string : the name of the file\n",
    "    outputs(6):\n",
    "        2D numpy array with the data\n",
    "    \"\"\"\n",
    "    # Calculate the medians\n",
    "    agem,fnlwgtm,edunm,capgm,caplm,hpwm = medians(file)\n",
    "    fin = open(file,\"r\")\n",
    "    reader = csv.reader(fin)\n",
    "    data = []\n",
    "    total = 0\n",
    "    for row in reader:\n",
    "        total+=1\n",
    "        # Skip line 0 in the file\n",
    "        if(total==1):\n",
    "            continue\n",
    "        l = [x.lstrip().rstrip() for x in row]\n",
    "        t = [0 for i in range(15)]\n",
    "        \n",
    "        # Encode the categorical attributes\n",
    "        t[0] = encode[\"rich\"][l[-1]]; t[2] = encode[\"wc\"][l[1]]; t[4] = encode[\"edu\"][l[3]]\n",
    "        t[6] = encode[\"mar\"][l[5]]; t[7] = encode[\"occ\"][l[6]]; t[8] = encode[\"rel\"][l[7]]\n",
    "        t[9] = encode[\"race\"][l[8]]; t[10] = encode[\"sex\"][l[9]]; t[14] = encode[\"nc\"][l[13]]\n",
    "        \n",
    "        # Binarize the numerical attributes based on median.\n",
    "        # Modify this section to read the file in part c where you split the continuos attributes baed on dynamic median values.\n",
    "        t[1] = float(l[0])>=agem; t[3] = float(l[2])>=fnlwgtm; t[5] = float(l[4])>=edunm;\n",
    "        t[11] = float(l[10])>=capgm; t[12] = float(l[11])>=caplm; t[13] = float(l[12])>=hpwm;\n",
    "        \n",
    "        # Convert some of the booleans to ints\n",
    "        data.append([int(x) for x in t])\n",
    "    \n",
    "    return np.array(data,dtype=np.int64)\n",
    "\n",
    "## Read the data\n",
    "train_data = preprocess(\"dtree_data/train.csv\")\n",
    "valid_data = preprocess(\"dtree_data/valid.csv\")\n",
    "test_data = preprocess(\"dtree_data/test.csv\")\n",
    "\n",
    "print(\"The sizes are \",\"Train:\",len(train_data),\", Validation:\",len(valid_data),\", Test:\",len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T04:53:50.434189Z",
     "start_time": "2018-04-03T04:53:49.890904Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "    def __init__(self):\n",
    "        self.am_leaf = False\n",
    "        self.children = {} # feature_value:Node\n",
    "        self.feature_index = -1\n",
    "        self.Y_val = -1\n",
    "        self.Y_confidence = -1\n",
    "    def fit(self,X,Y):\n",
    "        all_same_class = np.all(Y==Y[0])\n",
    "        if all_same_class:\n",
    "            self.am_leaf = True\n",
    "            self.Y_val = Y[0]\n",
    "            self.Y_confidence = 1\n",
    "        else:\n",
    "            self.am_leaf = False\n",
    "            Y_vals , Y_counts = np.unique(Y,return_counts=True)\n",
    "            if Y_counts[0] >= Y_counts[1]:\n",
    "                self.Y_val = Y_vals[0]\n",
    "                self.Y_confidence = Y_counts[0] / (Y_counts[0] + Y_counts[1])\n",
    "            else:\n",
    "                self.Y_val = Y_vals[1]\n",
    "                self.Y_confidence = Y_counts[1] / (Y_counts[0] + Y_counts[1])\n",
    "            info_gain_list = [self.info_gain(X[:,ind],Y) for ind in range(0,X.shape[-1])]\n",
    "            max_info_gain = max(info_gain_list)\n",
    "            if max_info_gain <= 1e-8:\n",
    "                self.am_leaf = True\n",
    "                return\n",
    "            max_index = info_gain_list.index(max_info_gain)\n",
    "            X_column = X[:,max_index]\n",
    "            X_unique, X_counts = np.unique(X_column,return_counts=True)\n",
    "            self.feature_index = max_index\n",
    "            for X_val, X_count in zip(X_unique,X_counts):\n",
    "                subset_boolean = X_column==X_val\n",
    "                small_Y = Y[subset_boolean]\n",
    "                small_X = X[subset_boolean]\n",
    "                child = DecisionTreeNode()\n",
    "                self.children[X_val] = child\n",
    "                child.fit(small_X,small_Y)     \n",
    "    def predict_single(self,x):\n",
    "        if self.am_leaf:\n",
    "            return self.Y_val\n",
    "        else:\n",
    "            try:\n",
    "                child_node = self.children[x[self.feature_index]]\n",
    "                return child_node.predict_single(x)\n",
    "            except KeyError:\n",
    "                zero_prob = 0\n",
    "                one_prob = 0\n",
    "                for _,node in self.children.items():\n",
    "                    if node.predict_single(x) == 0:\n",
    "                        zero_prob += node.Y_confidence\n",
    "                    else:\n",
    "                        one_prob += node.Y_confidence\n",
    "#                 print(\"Zero prob is \",zero_prob,\" and one is \",one_prob)\n",
    "                if zero_prob >= one_prob:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 1\n",
    "    def info_gain(self,X_column,Y):\n",
    "        _ , Y_counts = np.unique(Y,return_counts=True)\n",
    "        num_examples = Y.shape[0]\n",
    "        initial_entropy = self.entropy(Y_counts)\n",
    "        X_unique, X_counts = np.unique(X_column,return_counts=True)\n",
    "        new_entropy = 0\n",
    "        for X_val, X_count in zip(X_unique,X_counts):\n",
    "            subset_boolean = X_column==X_val\n",
    "            small_Y = Y[subset_boolean]\n",
    "            _ , small_counts = np.unique(small_Y,return_counts=True)\n",
    "            new_entropy += X_count*self.entropy(small_counts)/num_examples\n",
    "        info_gain = initial_entropy - new_entropy\n",
    "        return info_gain\n",
    "    def entropy (self,arr):\n",
    "        n_elements = np.sum(arr)\n",
    "        ent = 0\n",
    "        for el in arr:\n",
    "            ent += -(el/n_elements)*math.log(el/n_elements)\n",
    "        return ent\n",
    "    def partial_fit_to_depth(self,X,Y,depth_target,depth_current):\n",
    "        all_same_class = np.all(Y==Y[0])\n",
    "        if all_same_class:\n",
    "            self.am_leaf = True\n",
    "            self.Y_val = Y[0]\n",
    "            self.Y_confidence = 1\n",
    "        else:\n",
    "            self.am_leaf = False\n",
    "            Y_vals , Y_counts = np.unique(Y,return_counts=True)\n",
    "            if Y_counts[0] >= Y_counts[1]:\n",
    "                self.Y_val = Y_vals[0]\n",
    "                self.Y_confidence = Y_counts[0] / (Y_counts[0] + Y_counts[1])\n",
    "            else:\n",
    "                self.Y_val = Y_vals[1]\n",
    "                self.Y_confidence = Y_counts[1] / (Y_counts[0] + Y_counts[1])\n",
    "            if depth_target == depth_current:\n",
    "                return\n",
    "            info_gain_list = [self.info_gain(X[:,ind],Y) for ind in range(0,X.shape[-1])]\n",
    "            max_info_gain = max(info_gain_list)\n",
    "            if max_info_gain <= 1e-8:\n",
    "                self.am_leaf = True\n",
    "                return\n",
    "            max_index = info_gain_list.index(max_info_gain)\n",
    "            X_column = X[:,max_index]\n",
    "            X_unique, X_counts = np.unique(X_column,return_counts=True)\n",
    "            self.feature_index = max_index\n",
    "            for X_val, X_count in zip(X_unique,X_counts):\n",
    "                subset_boolean = X_column==X_val\n",
    "                small_Y = Y[subset_boolean]\n",
    "                small_X = X[subset_boolean]\n",
    "                child = DecisionTreeNode()\n",
    "                self.children[X_val] = child\n",
    "                \n",
    "                child.partial_fit_to_depth(small_X,small_Y,depth_target,depth_current+1)     \n",
    "    def partial_predict_to_depth(self,x):\n",
    "        if self.am_leaf:\n",
    "            return self.Y_val\n",
    "        else:\n",
    "            if len(self.children) == 0:\n",
    "                return self.Y_val\n",
    "            try:\n",
    "                child_node = self.children[x[self.feature_index]]\n",
    "                return child_node.predict_single(x)\n",
    "            except KeyError:\n",
    "                zero_prob = 0\n",
    "                one_prob = 0\n",
    "                for _,node in self.children.items():\n",
    "                    if node.predict_single(x) == 0:\n",
    "                        zero_prob += node.Y_confidence\n",
    "                    else:\n",
    "                        one_prob += node.Y_confidence\n",
    "#                 print(\"Zero prob is \",zero_prob,\" and one is \",one_prob)\n",
    "                if zero_prob >= one_prob:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T04:53:50.484187Z",
     "start_time": "2018-04-03T04:53:50.458321Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    Assumes X has discrete features represented by arbitrary integers and \n",
    "    Y is a boolean target variable represented as an array of 0's and 1's\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.root = DecisionTreeNode()\n",
    "    def fit(self,X,Y):\n",
    "        self.root.fit(X,Y)\n",
    "    def predict(self,X):\n",
    "        pred = []\n",
    "        for example in X:\n",
    "            pred.append(self.root.predict_single(example))\n",
    "        return np.asarray(pred)\n",
    "    def partial_fit_to_depth(self,X,Y,depth):\n",
    "        self.root.partial_fit_to_depth(X,Y,depth,1)\n",
    "    def partial_predict_to_depth(self,X):\n",
    "        pred = []\n",
    "        for example in X:\n",
    "            pred.append(self.root.partial_predict_to_depth(example))\n",
    "        return np.asarray(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T07:56:25.107718Z",
     "start_time": "2018-04-03T07:56:25.100713Z"
    }
   },
   "outputs": [],
   "source": [
    "train_X = train_data[:,1:]\n",
    "train_Y = train_data[:,0]\n",
    "valid_X = valid_data[:,1:]\n",
    "valid_Y = valid_data[:,0]\n",
    "test_X = test_data[:,1:]\n",
    "test_Y = test_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T07:56:26.283898Z",
     "start_time": "2018-04-03T07:56:26.281187Z"
    }
   },
   "outputs": [],
   "source": [
    "myclassifier = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T07:56:32.425917Z",
     "start_time": "2018-04-03T07:56:26.707302Z"
    }
   },
   "outputs": [],
   "source": [
    "myclassifier.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T07:56:32.447983Z",
     "start_time": "2018-04-03T07:56:32.441439Z"
    }
   },
   "outputs": [],
   "source": [
    "def count(node):\n",
    "    if node.am_leaf:\n",
    "        return 1\n",
    "    else:\n",
    "        c = 1\n",
    "        for _,ch in node.children.items():\n",
    "            c += count(ch)\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T07:56:32.464649Z",
     "start_time": "2018-04-03T07:56:32.462605Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T07:55:51.135315Z",
     "start_time": "2018-04-03T07:55:51.115647Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_pred = myclassifier.predict(valid_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T07:55:51.630690Z",
     "start_time": "2018-04-03T07:55:51.625134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.88      0.87      2236\n",
      "          1       0.62      0.57      0.59       764\n",
      "\n",
      "avg / total       0.80      0.80      0.80      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valid_Y,valid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T07:55:53.100443Z",
     "start_time": "2018-04-03T07:55:53.095533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.801"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(valid_Y,valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T04:57:46.752232Z",
     "start_time": "2018-04-03T04:56:42.205080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count is  1  Accuracy is  0.7453333333333333\n",
      "Count is  7  Accuracy is  0.7453333333333333\n",
      "Count is  92  Accuracy is  0.7453333333333333\n",
      "Count is  709  Accuracy is  0.7463333333333333\n",
      "Count is  2080  Accuracy is  0.7466666666666667\n",
      "Count is  3414  Accuracy is  0.7466666666666667\n",
      "Count is  4721  Accuracy is  0.745\n",
      "Count is  5951  Accuracy is  0.7496666666666667\n",
      "Count is  6954  Accuracy is  0.751\n",
      "Count is  7625  Accuracy is  0.7646666666666667\n",
      "Count is  7833  Accuracy is  0.7986666666666666\n",
      "Count is  7898  Accuracy is  0.799\n",
      "Count is  7898  Accuracy is  0.801\n",
      "Count is  7898  Accuracy is  0.801\n",
      "Count is  7898  Accuracy is  0.801\n",
      "Count is  7898  Accuracy is  0.801\n",
      "Count is  7898  Accuracy is  0.801\n",
      "Count is  7898  Accuracy is  0.801\n",
      "Count is  7898  Accuracy is  0.801\n"
     ]
    }
   ],
   "source": [
    "for dep in range(1,18):\n",
    "    myclassifier = DecisionTreeClassifier()\n",
    "    myclassifier.partial_fit_to_depth(train_X,train_Y,dep)\n",
    "    valid_pred = myclassifier.partial_predict_to_depth(valid_X)\n",
    "    val_acc = accuracy_score(valid_Y,valid_pred)\n",
    "    print(\"Depth is \",dep,\" Count is \",count(myclassifier.root),\" Accuracy is \",val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T08:25:47.619543Z",
     "start_time": "2018-04-03T08:25:47.496627Z"
    }
   },
   "outputs": [],
   "source": [
    "# assume fitted with partial fit and will be predicted with partial predict\n",
    "def prune_decision_tree_node_once(node,val_x,val_y):\n",
    "    # prune once and return (node,increase_in_acc_after_pruning)\n",
    "    if (len(node.children) == 0) or (val_y.shape[0]==0):\n",
    "        return (node,0)\n",
    "    else:\n",
    "        children_tuples = []\n",
    "        for feat_val,ch in node.children.items():\n",
    "            ind = val_x[:,node.feature_index] == feat_val\n",
    "            children_tuples.append(prune_decision_tree_node_once(ch,val_x[ind,:],val_y[ind]))\n",
    "        pred = []\n",
    "        for example in val_x:\n",
    "            pred.append(node.partial_predict_to_depth(example))\n",
    "        pred = np.asarray(pred)\n",
    "        pred_pruned = np.asarray([node.Y_val for _ in range(val_y.shape[0])])\n",
    "        pred_accuracy = np.sum(pred == val_y)/val_y.shape[0]\n",
    "        pred_pruned_accuracy = np.sum(pred_pruned == val_y)/val_y.shape[0]\n",
    "        incr = pred_pruned_accuracy - pred_accuracy\n",
    "        max_inc = -1\n",
    "        for tup in children_tuples:\n",
    "            if tup[1] > max_inc:\n",
    "                max_inc = tup[1]\n",
    "                max_node = tup[0]\n",
    "        if incr>max_inc:\n",
    "            max_inc = incr\n",
    "            max_node = node\n",
    "        return (max_node,max_inc) \n",
    "def prune_at_node(subtree_root_node,node_to_prune_at):\n",
    "    if subtree_root_node == node_to_prune_at:\n",
    "        subtree_root_node.children = {}\n",
    "        subtree_root_node.feature_index  = -1\n",
    "        return True\n",
    "    else:\n",
    "        done = False\n",
    "        for _,child in subtree_root_node.children.items():\n",
    "            done = done or prune_at_node(child,node_to_prune_at)\n",
    "            if done:\n",
    "                return True\n",
    "def prune_decision_tree_classifier(classifier,val_X,val_Y):\n",
    "    root_node = classifier.root\n",
    "    increase = 1\n",
    "    while increase > 0.25:\n",
    "        node_to_prune_at, increase = prune_decision_tree_node_once(root_node,val_X,val_Y)\n",
    "        prune_at_node(root_node,node_to_prune_at)\n",
    "#         print(\"Pruned once with increase \",increase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T08:25:53.638004Z",
     "start_time": "2018-04-03T08:25:47.724050Z"
    }
   },
   "outputs": [],
   "source": [
    "myclassifier = DecisionTreeClassifier()\n",
    "myclassifier.partial_fit_to_depth(train_X,train_Y,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T08:25:53.725119Z",
     "start_time": "2018-04-03T08:25:53.660046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of nodes is  7898  Val Acc is  0.801  and test Acc is  0.8055714285714286\n"
     ]
    }
   ],
   "source": [
    "valid_pred = myclassifier.predict(valid_X)\n",
    "val_acc = accuracy_score(valid_Y,valid_pred)\n",
    "test_pred = myclassifier.predict(test_X)\n",
    "test_acc = accuracy_score(test_Y,test_pred)\n",
    "print(\"No. of nodes is \",count(myclassifier.root),\" Val Acc is \",val_acc,\" and test Acc is \",test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T08:26:09.812040Z",
     "start_time": "2018-04-03T08:25:53.745847Z"
    }
   },
   "outputs": [],
   "source": [
    "prune_decision_tree_classifier(myclassifier,valid_X,valid_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T08:26:09.873828Z",
     "start_time": "2018-04-03T08:26:09.828085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of nodes is  7425  Val Acc is  0.8273333333333334  and test Acc is  0.808\n"
     ]
    }
   ],
   "source": [
    "valid_pred = myclassifier.predict(valid_X)\n",
    "val_acc = accuracy_score(valid_Y,valid_pred)\n",
    "test_pred = myclassifier.predict(test_X)\n",
    "test_acc = accuracy_score(test_Y,test_pred)\n",
    "print(\"No. of nodes is \",count(myclassifier.root),\" Val Acc is \",val_acc,\" and test Acc is \",test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note : See why changing increase threshold to very low actually decreases validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T09:10:37.481276Z",
     "start_time": "2018-04-03T09:10:36.391068Z"
    }
   },
   "outputs": [],
   "source": [
    "## The possible attributes in the data with the prediction at index 0. Smaller names for brevity.\n",
    "attributes = [\"rich\",\"age\",\"wc\",\"fnlwgt\",\"edu\",\"edun\",\"mar\",\"occ\",\"rel\",\"race\",\"sex\",\"capg\",\"canpl\",\"hpw\",\"nc\"]\n",
    "\n",
    "## Get the encoding of the csv file by replacing each categorical attribute value by its index.\n",
    "wc_l = \"Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked\".split(\", \")\n",
    "edu_l = \"Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool\".split(\", \")\n",
    "mar_l = \"Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse\".split(\", \")\n",
    "occ_l = \"Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces\".split(\", \")\n",
    "rel_l = \"Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\".split(\", \")\n",
    "race_l = \"White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\".split(\", \")\n",
    "sex_l = \"Female, Male\".split(\", \")\n",
    "nc_l = \"United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands\".split(\", \")\n",
    "encode = {\n",
    "    \"rich\"   : {\"0\":0,\"1\":1},\n",
    "    \"wc\"     : {wc_l[i]:i for i in range(len(wc_l))},\n",
    "    \"edu\"    : {edu_l[i]:i for i in range(len(edu_l))},\n",
    "    \"mar\"    : {mar_l[i]:i for i in range(len(mar_l))},\n",
    "    \"occ\"    : {occ_l[i]:i for i in range(len(occ_l))},\n",
    "    \"rel\"    : {rel_l[i]:i for i in range(len(rel_l))},\n",
    "    \"race\"   : {race_l[i]:i for i in range(len(race_l))},\n",
    "    \"sex\"    : {sex_l[i]:i for i in range(len(sex_l))},\n",
    "    \"nc\"     : {nc_l[i]:i for i in range(len(nc_l))},\n",
    "    }\n",
    "\n",
    "def medians(file):\n",
    "    \"\"\"\n",
    "    Given a csv file, find the medians of the categorical attributes for the whole data.\n",
    "    params(1): \n",
    "        file : string : the name of the file\n",
    "    outputs(6):\n",
    "        median values for the categorical columns\n",
    "    \"\"\"\n",
    "    fin = open(file,\"r\")\n",
    "    reader = csv.reader(fin)\n",
    "    age, fnlwgt, edun, capg, capl, hpw = ([] for i in range(6))\n",
    "    total = 0\n",
    "    for row in reader:\n",
    "        total+=1\n",
    "        if(total==1):\n",
    "            continue\n",
    "        l = [x.lstrip().rstrip() for x in row]\n",
    "        age.append(int(l[0]));\n",
    "        fnlwgt.append(int(l[2]));\n",
    "        edun.append(int(l[4]));\n",
    "        capg.append(int(l[10]));\n",
    "        capl.append(int(l[11]));\n",
    "        hpw.append(int(l[12]));\n",
    "    fin.close()\n",
    "    return(statistics.median(age),statistics.median(fnlwgt),statistics.median(edun),statistics.median(capg),statistics.median(capl),statistics.median(hpw))\n",
    "\n",
    "def preprocess(file):\n",
    "    \"\"\"\n",
    "    Given a file, read its data by encoding categorical attributes and binarising continuos attributes based on median.\n",
    "    params(1): \n",
    "        file : string : the name of the file\n",
    "    outputs(6):\n",
    "        2D numpy array with the data\n",
    "    \"\"\"\n",
    "    # Calculate the medians\n",
    "    agem,fnlwgtm,edunm,capgm,caplm,hpwm = medians(file)\n",
    "    fin = open(file,\"r\")\n",
    "    reader = csv.reader(fin)\n",
    "    data = []\n",
    "    total = 0\n",
    "    for row in reader:\n",
    "        total+=1\n",
    "        # Skip line 0 in the file\n",
    "        if(total==1):\n",
    "            continue\n",
    "        l = [x.lstrip().rstrip() for x in row]\n",
    "        t = [0 for i in range(15)]\n",
    "        \n",
    "        # Encode the categorical attributes\n",
    "        t[0] = encode[\"rich\"][l[-1]]; t[2] = encode[\"wc\"][l[1]]; t[4] = encode[\"edu\"][l[3]]\n",
    "        t[6] = encode[\"mar\"][l[5]]; t[7] = encode[\"occ\"][l[6]]; t[8] = encode[\"rel\"][l[7]]\n",
    "        t[9] = encode[\"race\"][l[8]]; t[10] = encode[\"sex\"][l[9]]; t[14] = encode[\"nc\"][l[13]]\n",
    "        \n",
    "        # Binarize the numerical attributes based on median.\n",
    "        # Modify this section to read the file in part c where you split the continuos attributes baed on dynamic median values.\n",
    "        t[1] = float(l[0]); t[3] = float(l[2]); t[5] = float(l[4]);\n",
    "        t[11] = float(l[10]); t[12] = float(l[11]); t[13] = float(l[12]);\n",
    "        \n",
    "        # Convert some of the booleans to ints\n",
    "        data.append([int(x) for x in t])\n",
    "    \n",
    "    return np.array(data,dtype=np.int64)\n",
    "\n",
    "## Read the data\n",
    "train_data_3 = preprocess(\"dtree_data/train.csv\")\n",
    "valid_data_3 = preprocess(\"dtree_data/valid.csv\")\n",
    "test_data_3 = preprocess(\"dtree_data/test.csv\")\n",
    "\n",
    "numerical_indices = [0,2,4,10,11,12]\n",
    "train_X_3 = train_data_3[:,1:]\n",
    "train_Y_3 = train_data_3[:,0]\n",
    "valid_X_3 = valid_data_3[:,1:]\n",
    "valid_Y_3 = valid_data_3[:,0]\n",
    "test_X_3 = test_data_3[:,1:]\n",
    "test_Y_3 = test_data_3[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T09:18:55.552183Z",
     "start_time": "2018-04-03T09:18:54.226047Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTreeNode_3:\n",
    "    def __init__(self):\n",
    "        self.am_leaf = False\n",
    "        self.children = {} # feature_value:Node\n",
    "        self.feature_index = -1\n",
    "        self.Y_val = -1\n",
    "        self.Y_confidence = -1\n",
    "        self.is_split_on_numeric = False\n",
    "        self.median = -1\n",
    "    def fit(self,X,Y):\n",
    "        all_same_class = np.all(Y==Y[0])\n",
    "        if all_same_class:\n",
    "            self.am_leaf = True\n",
    "            self.Y_val = Y[0]\n",
    "            self.Y_confidence = 1\n",
    "        else:\n",
    "            self.am_leaf = False\n",
    "            Y_vals , Y_counts = np.unique(Y,return_counts=True)\n",
    "            if Y_counts[0] >= Y_counts[1]:\n",
    "                self.Y_val = Y_vals[0]\n",
    "                self.Y_confidence = Y_counts[0] / (Y_counts[0] + Y_counts[1])\n",
    "            else:\n",
    "                self.Y_val = Y_vals[1]\n",
    "                self.Y_confidence = Y_counts[1] / (Y_counts[0] + Y_counts[1])\n",
    "            info_gain_list = []\n",
    "            for ind in range(0,X.shape[-1]):\n",
    "                if ind in numerical_indices:\n",
    "                    indicator = (X[:,ind] >= np.median(X[:,ind])).astype(int)\n",
    "                    info_gain_list.append(self.info_gain(indicator,Y))\n",
    "                else:\n",
    "                    info_gain_list.append(self.info_gain(X[:,ind],Y) )\n",
    "            max_info_gain = max(info_gain_list)\n",
    "            if max_info_gain <= 1e-8:\n",
    "                self.am_leaf = True\n",
    "                return\n",
    "            max_index = info_gain_list.index(max_info_gain)\n",
    "            X_column = X[:,max_index]\n",
    "            self.feature_index = max_index\n",
    "            if max_index in numerical_indices:\n",
    "                self.is_split_on_numeric = True\n",
    "                med = np.median(X[:,max_index])\n",
    "                self.median = med\n",
    "                indicator_high = X[:,max_index] >= med\n",
    "                indicator_low = X[:,max_index] < med\n",
    "                low_x ,low_y = X[indicator_low], Y[indicator_low]\n",
    "                high_x ,high_y = X[indicator_high], Y[indicator_high]\n",
    "                child_low =  DecisionTreeNode_3()\n",
    "                child_high =  DecisionTreeNode_3()\n",
    "                child_low.fit(low_x ,low_y)\n",
    "                child_high.fit(high_x ,high_y)\n",
    "                self.children[\"higher\"] = child_high\n",
    "                self.children[\"lower\"] = child_low\n",
    "            else:\n",
    "                X_unique, X_counts = np.unique(X_column,return_counts=True)\n",
    "                for X_val, X_count in zip(X_unique,X_counts):\n",
    "                    subset_boolean = X_column==X_val\n",
    "                    small_Y = Y[subset_boolean]\n",
    "                    small_X = X[subset_boolean]\n",
    "                    child = DecisionTreeNode_3()\n",
    "                    self.children[X_val] = child\n",
    "                    child.fit(small_X,small_Y)             \n",
    "                \n",
    "    def predict_single(self,x):\n",
    "        if self.am_leaf:\n",
    "            return self.Y_val\n",
    "        else:\n",
    "            if self.is_split_on_numeric:\n",
    "                if x[self.feature_index] >= self.median:\n",
    "                    child_node = self.children[\"higher\"]\n",
    "                    return child_node.predict_single(x)    \n",
    "                else: \n",
    "                    child_node = self.children[\"lower\"]\n",
    "                    return child_node.predict_single(x)\n",
    "            else:\n",
    "                try:\n",
    "                    child_node = self.children[x[self.feature_index]]\n",
    "                    return child_node.predict_single(x)\n",
    "                except KeyError:\n",
    "                    zero_prob = 0\n",
    "                    one_prob = 0\n",
    "                    for _,node in self.children.items():\n",
    "                        if node.predict_single(x) == 0:\n",
    "                            zero_prob += node.Y_confidence\n",
    "                        else:\n",
    "                            one_prob += node.Y_confidence\n",
    "                    if zero_prob >= one_prob:\n",
    "                        return 0\n",
    "                    else:\n",
    "                        return 1\n",
    "    def info_gain(self,X_column,Y):\n",
    "        _ , Y_counts = np.unique(Y,return_counts=True)\n",
    "        num_examples = Y.shape[0]\n",
    "        initial_entropy = self.entropy(Y_counts)\n",
    "        X_unique, X_counts = np.unique(X_column,return_counts=True)\n",
    "        new_entropy = 0\n",
    "        for X_val, X_count in zip(X_unique,X_counts):\n",
    "            subset_boolean = X_column==X_val\n",
    "            small_Y = Y[subset_boolean]\n",
    "            _ , small_counts = np.unique(small_Y,return_counts=True)\n",
    "            new_entropy += X_count*self.entropy(small_counts)/num_examples\n",
    "        info_gain = initial_entropy - new_entropy\n",
    "        return info_gain\n",
    "    def entropy (self,arr):\n",
    "        n_elements = np.sum(arr)\n",
    "        ent = 0\n",
    "        for el in arr:\n",
    "            ent += -(el/n_elements)*math.log(el/n_elements)\n",
    "        return ent\n",
    "    def partial_fit_to_depth(self,X,Y,depth_target,depth_current):\n",
    "        all_same_class = np.all(Y==Y[0])\n",
    "        if all_same_class:\n",
    "            self.am_leaf = True\n",
    "            self.Y_val = Y[0]\n",
    "            self.Y_confidence = 1\n",
    "        else:\n",
    "            self.am_leaf = False\n",
    "            Y_vals , Y_counts = np.unique(Y,return_counts=True)\n",
    "            if Y_counts[0] >= Y_counts[1]:\n",
    "                self.Y_val = Y_vals[0]\n",
    "                self.Y_confidence = Y_counts[0] / (Y_counts[0] + Y_counts[1])\n",
    "            else:\n",
    "                self.Y_val = Y_vals[1]\n",
    "                self.Y_confidence = Y_counts[1] / (Y_counts[0] + Y_counts[1])\n",
    "            if depth_target == depth_current:\n",
    "                return\n",
    "            \n",
    "            info_gain_list = []\n",
    "            for ind in range(0,X.shape[-1]):\n",
    "                if ind in numerical_indices:\n",
    "                    indicator = X[:,ind] >= np.median(X[:,ind])\n",
    "                    indicator_int = indicator.astype(int)\n",
    "                    info_gain_list.append(self.info_gain(indicator_int,Y))\n",
    "                else:\n",
    "                    info_gain_list.append(self.info_gain(X[:,ind],Y) )\n",
    "            max_info_gain = max(info_gain_list)\n",
    "            if max_info_gain <= 1e-8:\n",
    "                self.am_leaf = True\n",
    "                return\n",
    "            max_index = info_gain_list.index(max_info_gain)\n",
    "            X_column = X[:,max_index]\n",
    "            self.feature_index = max_index\n",
    "            if max_index in numerical_indices:\n",
    "                self.is_split_on_numeric = True\n",
    "                med = np.median(X[:,max_index])\n",
    "                self.median = med\n",
    "                indicator_high = X[:,max_index] >= med\n",
    "                indicator_low = X[:,max_index] < med\n",
    "                low_x ,low_y = X[indicator_low], Y[indicator_low]\n",
    "                high_x ,high_y = X[indicator_high], Y[indicator_high]\n",
    "                child_low =  DecisionTreeNode_3()\n",
    "                child_high =  DecisionTreeNode_3()\n",
    "                child_low.partial_fit_to_depth(low_x ,low_y,depth_target,depth_current+1)\n",
    "                child_high.partial_fit_to_depth(high_x ,high_y,depth_target,depth_current+1)\n",
    "                self.children[\"higher\"] = child_high\n",
    "                self.children[\"lower\"] = child_low\n",
    "            else:\n",
    "                X_unique, X_counts = np.unique(X_column,return_counts=True)\n",
    "                for X_val, X_count in zip(X_unique,X_counts):\n",
    "                    subset_boolean = X_column==X_val\n",
    "                    small_Y = Y[subset_boolean]\n",
    "                    small_X = X[subset_boolean]\n",
    "                    child = DecisionTreeNode_3()\n",
    "                    self.children[X_val] = child\n",
    "                    child.partial_fit_to_depth(small_X,small_Y,depth_target,depth_current+1)     \n",
    "    def partial_predict_to_depth(self,x):\n",
    "        if self.am_leaf or (len(self.children) == 0):\n",
    "            return self.Y_val\n",
    "        else:\n",
    "            if self.is_split_on_numeric:\n",
    "                if x[self.feature_index] >= self.median:\n",
    "                    child_node = self.children[\"higher\"]\n",
    "                    return child_node.predict_single(x)    \n",
    "                else: \n",
    "                    child_node = self.children[\"lower\"]\n",
    "                    return child_node.predict_single(x)\n",
    "            else:\n",
    "                try:\n",
    "                    child_node = self.children[x[self.feature_index]]\n",
    "                    return child_node.predict_single(x)\n",
    "                except KeyError:\n",
    "                    zero_prob = 0\n",
    "                    one_prob = 0\n",
    "                    for _,node in self.children.items():\n",
    "                        if node.predict_single(x) == 0:\n",
    "                            zero_prob += node.Y_confidence\n",
    "                        else:\n",
    "                            one_prob += node.Y_confidence\n",
    "                    if zero_prob >= one_prob:\n",
    "                        return 0\n",
    "                    else:\n",
    "                        return 1\n",
    "\n",
    "class DecisionTreeClassifier_3:\n",
    "    \"\"\"\n",
    "    Assumes X has discrete features represented by arbitrary integers along\n",
    "    with continuous features and Y is a boolean target variable\n",
    "    represented as an array of 0's and 1's\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.root = DecisionTreeNode_3()\n",
    "    def fit(self,X,Y):\n",
    "        self.root.fit(X,Y)\n",
    "    def predict(self,X):\n",
    "        pred = []\n",
    "        for example in X:\n",
    "            pred.append(self.root.predict_single(example))\n",
    "        return np.asarray(pred)\n",
    "    def partial_fit_to_depth(self,X,Y,depth):\n",
    "        self.root.partial_fit_to_depth(X,Y,depth,1)\n",
    "    def partial_predict_to_depth(self,X):\n",
    "        pred = []\n",
    "        for example in X:\n",
    "            pred.append(self.root.partial_predict_to_depth(example))\n",
    "        return np.asarray(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T09:19:38.368054Z",
     "start_time": "2018-04-03T09:19:27.069148Z"
    }
   },
   "outputs": [],
   "source": [
    "myclassifier = DecisionTreeClassifier_3()\n",
    "myclassifier.partial_fit_to_depth(train_X_3,train_Y_3,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T09:19:38.552336Z",
     "start_time": "2018-04-03T09:19:38.389658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of nodes is  15359  Val Acc is  0.79  and test Acc is  0.78\n"
     ]
    }
   ],
   "source": [
    "valid_pred = myclassifier.partial_predict_to_depth(valid_X_3)\n",
    "val_acc = accuracy_score(valid_Y_3,valid_pred)\n",
    "test_pred = myclassifier.partial_predict_to_depth(test_X_3)\n",
    "test_acc = accuracy_score(test_Y_3,test_pred)\n",
    "print(\"No. of nodes is \",count(myclassifier.root),\" Val Acc is \",val_acc,\" and test Acc is \",test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T09:22:13.796104Z",
     "start_time": "2018-04-03T09:20:22.394926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth is  1  Count is  1  Accuracy is  0.7453333333333333\n",
      "Depth is  2  Count is  7  Accuracy is  0.7453333333333333\n",
      "Depth is  3  Count is  92  Accuracy is  0.7453333333333333\n",
      "Depth is  4  Count is  697  Accuracy is  0.7463333333333333\n",
      "Depth is  5  Count is  2081  Accuracy is  0.7476666666666667\n",
      "Depth is  6  Count is  3408  Accuracy is  0.749\n",
      "Depth is  7  Count is  4726  Accuracy is  0.7553333333333333\n",
      "Depth is  8  Count is  6085  Accuracy is  0.7573333333333333\n",
      "Depth is  9  Count is  7438  Accuracy is  0.763\n",
      "Depth is  10  Count is  8820  Accuracy is  0.768\n",
      "Depth is  11  Count is  10209  Accuracy is  0.7786666666666666\n",
      "Depth is  12  Count is  11596  Accuracy is  0.7833333333333333\n",
      "Depth is  13  Count is  12859  Accuracy is  0.7846666666666666\n",
      "Depth is  14  Count is  13839  Accuracy is  0.788\n",
      "Depth is  15  Count is  14539  Accuracy is  0.791\n",
      "Depth is  16  Count is  14981  Accuracy is  0.7916666666666666\n",
      "Depth is  17  Count is  15235  Accuracy is  0.7903333333333333\n",
      "Depth is  18  Count is  15331  Accuracy is  0.7903333333333333\n",
      "Depth is  19  Count is  15359  Accuracy is  0.79\n"
     ]
    }
   ],
   "source": [
    "for dep in range(1,20):\n",
    "    myclassifier = DecisionTreeClassifier_3()\n",
    "    myclassifier.partial_fit_to_depth(train_X_3,train_Y_3,dep)\n",
    "    valid_pred = myclassifier.partial_predict_to_depth(valid_X_3)\n",
    "    val_acc = accuracy_score(valid_Y_3,valid_pred)\n",
    "    print(\"Depth is \",dep,\" Count is \",count(myclassifier.root),\" val Accuracy is \",val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T09:25:03.970632Z",
     "start_time": "2018-04-03T09:25:03.967404Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as skCLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T09:29:34.923001Z",
     "start_time": "2018-04-03T09:29:34.825063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.8523333333333334\n"
     ]
    }
   ],
   "source": [
    "clf = skCLF(min_samples_leaf=6,min_samples_split=6,max_depth=10)\n",
    "clf.fit(train_X_3,train_Y_3)\n",
    "preds = clf.predict(valid_X_3)\n",
    "print(\"Accuracy is \",accuracy_score(valid_Y_3,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T09:30:24.320240Z",
     "start_time": "2018-04-03T09:30:24.313477Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T09:31:54.650192Z",
     "start_time": "2018-04-03T09:31:53.601133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.8563333333333333\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=50,min_samples_leaf=4,min_samples_split=5,max_depth=30)\n",
    "forest.fit(train_X_3,train_Y_3)\n",
    "preds = forest.predict(valid_X_3)\n",
    "print(\"Accuracy is \",accuracy_score(valid_Y_3,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
